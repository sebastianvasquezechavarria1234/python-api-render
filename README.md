
## üíª Correr el proyecto en local

**1. Clonar el repositorio**

git clone https://github.com/sebastianvasquezechavarria1234/python-api-render.git
cd api-python

2. Crear y activar el entorno virtual
python -m venv venv
venv\Scripts\activate

3. Instalar dependencias

pip install -r requirements.txt
4. Crear el archivo .env

GROQ_API_KEY=tu_clave_aqui
5. Correr la API

uvicorn app.main:app --reload
La API queda disponible en http://127.0.0.1:8000

# Chat IA ‚Äî Documentaci√≥n

## ¬øQu√© es este proyecto?

Es un chatbot con IA que funciona como intermediario entre el usuario y un modelo de lenguaje. El frontend (HTML + Tailwind) captura la pregunta del usuario y la env√≠a a una API propia construida en Python. Esa API se comunica con Groq usando el modelo LLaMA 3.3 70B para generar una respuesta inteligente y personalizada.

---

## ¬øC√≥mo funciona?

1. El usuario escribe una pregunta en el frontend
2. Vercel sirve el frontend (HTML + CSS + JS)
3. fetch() en JavaScript hace un POST a la API
4. FastAPI (Python) recibe la petici√≥n en el endpoint POST /ask
5. Groq API procesa la pregunta con LLaMA 3.3 70B
6. La respuesta regresa al frontend y se escribe con efecto typewriter

---

## Probar la API con Thunder Client

**M√©todo:** POST  
**URL:** https://python-api-render-ubr9.onrender.com/ask

**Body (JSON):**
```json
{
  "name": "Sebastian",
  "question": "¬øQu√© es la inteligencia artificial?",
  "personality": "casual"
}
Respuesta esperada:

json
{
  "answer": "¬°Hola Sebastian! La IA es..."
}
Valores disponibles para personality: casual, tutor, profesional, tecnico

 La API est√° en Render con plan gratuito. Si lleva tiempo inactiva, la primera petici√≥n puede tardar 30‚Äì60 segundos mientras el servidor se reactiva.

Stack tecnol√≥gico
Backend: Python + FastAPI

Frontend: HTML + Tailwind CSS

IA: Groq ‚Äî LLaMA 3.3 70B

Historial: localStorage

API Deploy: Render

Frontend Deploy: Vercel

Proceso de desarrollo
1. Crear la estructura del proyecto
Se cre√≥ la carpeta api-python/ con subcarpeta app/ que contiene main.py, schemas.py y groq_service.py.

2. Instalar dependencias
Se cre√≥ un entorno virtual y se instalaron: fastapi, uvicorn, groq, python-dotenv. Se gener√≥ el requirements.txt con pip freeze.

3. Construir la API en FastAPI
Se cre√≥ el endpoint POST /ask que recibe name, question y personality, y devuelve la respuesta del modelo como JSON.

4. Integrar Groq con system prompts
Se cre√≥ un diccionario de personalidades que cambia el system prompt seg√∫n la selecci√≥n del usuario, controlando el tono y comportamiento del modelo.

5. Construir el frontend
Se cre√≥ la interfaz con HTML + Tailwind CSS: modal de bienvenida, sidebar con historial, selector de personalidad, input de chat y efecto typewriter en las respuestas.

6. Historial con localStorage
Se implement√≥ persistencia de conversaciones en el navegador usando localStorage, permitiendo guardar, cargar y crear nuevos chats sin base de datos.

7. Variables de entorno
Se cre√≥ el archivo .env con la variable GROQ_API_KEY y se agreg√≥ al .gitignore para no exponer la clave en GitHub.

8. Despliegue en Render (API)
Se conect√≥ el repositorio a Render. Start Command: uvicorn app.main:app --host 0.0.0.0 --port 10000. Se configur√≥ la variable GROQ_API_KEY desde el panel de Render.

9. Despliegue en Vercel (Frontend)
Se conect√≥ el repositorio a Vercel y se configur√≥ el vercel.json para servir los archivos est√°ticos. Cada push a main redespliega autom√°ticamente.

Autor
Nombre: Sebastian Vasquez Echavarria

Portafolio: https://sebas-dev.vercel.app

GitHub: https://github.com/sebastianvasquezechavarria1234/python-api-render